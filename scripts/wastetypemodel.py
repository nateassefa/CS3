# -*- coding: utf-8 -*-
"""WasteTypeModel.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/11zSBBpVOtGAsvw7tYdgS4RR4GmCtJOrt

# WasteTypeModel - 3 Class CNN for Waste Classification

This notebook is the main starter code for the CS3 case study . It does the below:

- Loads the RealWasterDataset
- Regroups the 9 original labels into 3 classes (Recyclable, Compostable, Trash)
- Builds and trains a small CNN
- Evaluates the. model and prints metrics + a confusion matrix
"""

# 0. Imports & basic setup

import os
from collections import Counter

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

import tensorflow as tf
from tensorflow.keras import layers, models

from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, confusion_matrix

# Paths – adjust if your folder is somewhere else
DATA_DIR   = "../data/RealWaster"   # contains Cardboard/, Glass/, etc.
OUTPUT_DIR = "../output"
os.makedirs(OUTPUT_DIR, exist_ok=True)

IMG_SIZE   = (224, 224)  # a bit smaller than 224 to speed things up
BATCH_SIZE = 32
EPOCHS     = 8
SEED       = 42

# --------------------------------------------------
# 1. Map fine-grained labels → 3 super-classes
# --------------------------------------------------
FINE_TO_COARSE = {
    "Cardboard"        : "Recyclable",
    "Glass"            : "Recyclable",
    "Metal"            : "Recyclable",
    "Paper"            : "Recyclable",
    "Plastic"          : "Recyclable",
    "Food Organics"    : "Compostable",
    "Vegetation"       : "Compostable",
    "Miscellaneous Trash": "Trash",
    "Textile Trash"    : "Trash",
}

COARSE_CLASSES = ["Recyclable", "Compostable", "Trash"]
COARSE_TO_IDX  = {c: i for i, c in enumerate(COARSE_CLASSES)}

# --------------------------------------------------
# 2. Scan folders → list of image paths + 3-class labels
# --------------------------------------------------
def list_files_and_labels(base_dir):
    paths = []
    labels = []
    for fine_label in os.listdir(base_dir):
        fine_path = os.path.join(base_dir, fine_label)
        if not os.path.isdir(fine_path):
            continue
        if fine_label not in FINE_TO_COARSE:
            # Ignore unexpected folders
            print(f"Skipping unknown folder: {fine_label}")
            continue

        coarse_label = FINE_TO_COARSE[fine_label]
        for fname in os.listdir(fine_path):
            if fname.lower().endswith((".jpg", ".jpeg", ".png")):
                paths.append(os.path.join(fine_path, fname))
                labels.append(coarse_label)
    return np.array(paths), np.array(labels)

paths_all, labels_coarse = list_files_and_labels(DATA_DIR)
y_all = np.array([COARSE_TO_IDX[c] for c in labels_coarse])

print(f"Total images: {len(paths_all)}")
print("Coarse class distribution:", Counter(labels_coarse))

# --------------------------------------------------
# 3. Train / Val / Test split (stratified)
# --------------------------------------------------
# First split off test
X_temp, X_test, y_temp, y_test = train_test_split(
    paths_all, y_all, test_size=0.2, stratify=y_all, random_state=SEED
)
# Then split temp into train / val
X_train, X_val, y_train, y_val = train_test_split(
    X_temp, y_temp, test_size=0.2, stratify=y_temp, random_state=SEED
)

print("Train dist:", Counter(y_train))
print("Val dist  :", Counter(y_val))
print("Test dist :", Counter(y_test))
print("Label map :", COARSE_TO_IDX)

# --------------------------------------------------
# 4. tf.data pipeline
# --------------------------------------------------
def decode_img(path, label, augment=False):
    # read file
    img_bytes = tf.io.read_file(path)
    img = tf.image.decode_jpeg(img_bytes, channels=3)
    img = tf.image.convert_image_dtype(img, tf.float32)  # [0,1]
    img = tf.image.resize(img, IMG_SIZE)

    if augment:
        img = tf.image.random_flip_left_right(img)
        img = tf.image.random_flip_up_down(img)
        img = tf.image.random_brightness(img, 0.1)

    return img, label

def make_ds(paths, labels, augment=False, shuffle=False):
    ds = tf.data.Dataset.from_tensor_slices((paths, labels))
    if shuffle:
        ds = ds.shuffle(buffer_size=len(paths), seed=SEED)
    ds = ds.map(lambda p, y: decode_img(p, y, augment=augment),
                num_parallel_calls=tf.data.AUTOTUNE)
    ds = ds.batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)
    return ds

ds_train = make_ds(X_train, y_train, augment=True,  shuffle=True)
ds_val   = make_ds(X_val,   y_val,   augment=False, shuffle=False)
ds_test  = make_ds(X_test,  y_test,  augment=False, shuffle=False)

# --------------------------------------------------
# 5. Build a small CNN (3-class softmax)
# --------------------------------------------------
def build_small_cnn(input_shape=IMG_SIZE + (3,), n_classes=3):
    model = models.Sequential([
        layers.Input(shape=input_shape),

        layers.Conv2D(32, (3,3), activation="relu", padding="same"),
        layers.MaxPooling2D(),

        layers.Conv2D(64, (3,3), activation="relu", padding="same"),
        layers.MaxPooling2D(),

        layers.Conv2D(128, (3,3), activation="relu", padding="same"),
        layers.MaxPooling2D(),

        layers.Flatten(),
        layers.Dense(128, activation="relu"),
        layers.Dropout(0.5),
        layers.Dense(n_classes, activation="softmax")
    ])
    model.compile(
        optimizer=tf.keras.optimizers.Adam(1e-3),
        loss="sparse_categorical_crossentropy",
        metrics=["accuracy"]
    )
    return model

model = build_small_cnn(n_classes=len(COARSE_CLASSES))
model.summary()

# --------------------------------------------------
# 6. Train
# --------------------------------------------------
callbacks = [
    tf.keras.callbacks.EarlyStopping(
        monitor="val_loss", patience=4, restore_best_weights=True, verbose=1
    )
]

history = model.fit(
    ds_train,
    validation_data=ds_val,
    epochs=EPOCHS,
    callbacks=callbacks,
    verbose=2
)

# --------------------------------------------------
# 7. Evaluate on test set
# --------------------------------------------------
test_probs = model.predict(ds_test)
y_pred = np.argmax(test_probs, axis=1)

print("\nTest results:")
print("Accuracy:", (y_pred == y_test).mean())
print(classification_report(
    y_test, y_pred,
    target_names=COARSE_CLASSES,
    digits=4
))

# --------------------------------------------------
# 8. Confusion matrix plot
# --------------------------------------------------
cm = confusion_matrix(y_test, y_pred, labels=list(range(len(COARSE_CLASSES))))

fig, ax = plt.subplots(figsize=(5,4))
im = ax.imshow(cm, cmap="Blues")
ax.set_xticks(range(len(COARSE_CLASSES)))
ax.set_yticks(range(len(COARSE_CLASSES)))
ax.set_xticklabels(COARSE_CLASSES, rotation=45, ha="right")
ax.set_yticklabels(COARSE_CLASSES)
ax.set_xlabel("Predicted")
ax.set_ylabel("True")
ax.set_title("Confusion Matrix (3-class)")

for i in range(cm.shape[0]):
    for j in range(cm.shape[1]):
        ax.text(j, i, cm[i, j], ha="center", va="center", fontsize=9)

fig.colorbar(im, ax=ax, fraction=0.046, pad=0.04)
plt.tight_layout()

cm_path = os.path.join(OUTPUT_DIR, "confusion_matrix_3class.png")
plt.savefig(cm_path, dpi=300, bbox_inches="tight")
plt.show()
print(f"Saved confusion matrix to {cm_path}")